{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c7d9198",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Muhammad Omer Farooq Bhatti\"\n",
    "Id = '122498'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb65f770",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification.\n",
    "\n",
    "#### How is a Decision Tree fit?\n",
    "\n",
    "* Which variables to include on the tree?\n",
    "* How to choose the threshold?\n",
    "* When to stop the tree?\n",
    "\n",
    "**Key idea is that we want to choose the feature that has the lowest \"impurity\" to split our tree, thus our tree can reach the decision as fast as possible with smallest height possible**\n",
    "\n",
    "One way to measure impurity is using **Gini index** (another one is entropy but which measure very similar thing) with the following formula:\n",
    "\n",
    "$$ I_{G} = 1 - \\sum_{i=1}^{c} p_{i}^{2} $$\n",
    "\n",
    "where $c$ is number of classes, and $p_{i}$ is the probability of each class.  For example, let's say our X is <code>[[2],[3],[10],[19]]</code> and y is <code>[0, 0, 1, 1]</code>.  That is, if a node has 4 samples, and 2 samples are of class cancer, and 2 samples are of no cancer, then the probability of each class is\n",
    "\n",
    "$$p_{cancer}=(2/4)^2 = 0.25$$ and $$p_{no-cancer}=(2/4)^2 = 0.25$$   \n",
    "\n",
    "Thus the gini index of this node is\n",
    "\n",
    "$$ I_{G} = 1 - (0.25 + 0.25) = 0.5 $$\n",
    "\n",
    "Then we need to decide how to best split this node so we can get the lowest gini (highest purity) children.\n",
    "\n",
    "For example, if we split this sample with **x1 < 3**: we will get left node X as <code>[[2]]</code> and y as <code>[0]</code> and the right node X as <code>[[3],[10],[19]]</code> and y as <code>[0, 1, 1]</code>.  The weighted gini of the children are \n",
    "\n",
    "$$ 1/4*I_{leftG} + 3/4 * I_{rightG} =  $$\n",
    "$$ 1/4 * (1 - (1/1)^2) + 3/4 * (1 - (1/3)^2 - (2/3)^2) = 0.33 $$\n",
    "\n",
    "Hmm...but we know we can split better, right?  Let's try **x1 < 4**: we will get left node X as <code>[[2],[3]]</code> and y as <code>[0, 0]</code> and the right node X as <code>[[10],[19]]</code> and y as <code>[1, 1]</code>.  If you do the math right, the gini is 0!\n",
    "\n",
    "$$ 2/4 * (1 - (2/2)^2) + 2/4 * (1 - (2/2)^2 ) = 0 $$\n",
    "\n",
    "Thus, in conclusion, we can say that spliting **x1<4** is a much better split than **x1<3**.  In practice, to really find the best split, it is an exhaustive and greedy algorithm, in which we have to iterate and check every value on each feature as a candidate split, find the gini index.  \n",
    "\n",
    "#### How do we find all threshold for continuous values?\n",
    "\n",
    "We can sort all features.  Then we are identify critical value using the midpoint between all consecutive values.  For example, given X is <code>[[2],[3],[10],[19]]</code>, the critical value to compare is 2.5, 6.5 and 14.5.\n",
    "\n",
    "The code can be implemented in several ways.  Example are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8ccaf8",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b5c0b",
   "metadata": {},
   "source": [
    "One all value are exhausted, we can then use the best decision note as our split node.  Then when we go to the next node, we have to repeat again.  This algorithm is called **CART (Classification and Regression Trees)** algorithm, where the recursion keeps on going until certain stop criteria, such as maximum tree depth is reached, or no split can produce two children with lower purity.\n",
    "\n",
    "**Implementation steps:**\n",
    "\n",
    "1. Calculate the purity of the data\n",
    "2. Select a candidate split\n",
    "3. Calculate the purity of the data after the split\n",
    "4. Repeat for all variables\n",
    "5. Choose the variable with the lowest impurity \n",
    "6. Repeat for each split until some stop criteria is met\n",
    "\n",
    "Example could be stop criteria could be max tree depth, or minimum node records.\n",
    "\n",
    "Here are some snippets of the possible implementation \n",
    "of Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84471351",
   "metadata": {},
   "source": [
    "### When to use Decision Trees\n",
    "\n",
    "Decision Trees are more powerful than other classification in a sense that it can work very well given heterogenous features.  However, the downsides is high possibility of over-fitting: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from.\n",
    "\n",
    "However, by using information from multiple decision trees training on subset of data (i.e., random forests), we might expect better results.  We shall explore random forests and a general family of ensembles later in our course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40451150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c7a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To help with our implementation, we create a class Node\n",
    "class Node:\n",
    "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8324e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dtree:\n",
    "    def __init__(self, max_depth=0):\n",
    "        self.max_depth=max_depth\n",
    "        self.tree=[]\n",
    "        \n",
    "    def find_split(self, X, y, n_classes):\n",
    "        \"\"\" Find split where children has lowest impurity possible\n",
    "        in condition where the purity should also be less than the parent,\n",
    "        if not, stop.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples <= 1:\n",
    "            return None, None\n",
    "\n",
    "        #so it will not have any warning about \"referenced before assignments\"\n",
    "        feature_ix, threshold = None, None\n",
    "        #print(\"y :\" , y)\n",
    "\n",
    "        # Count of each class in the current node.\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(n_classes)] #[2, 2]\n",
    "        #print(\"sample_per_class_parent :\", sample_per_class_parent)\n",
    "\n",
    "        # Gini of parent node.\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\n",
    "\n",
    "        # Loop through all features.\n",
    "        for feature in range(n_features):\n",
    "\n",
    "            # Sort data along selected feature.\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\n",
    "            #print(\"sample_sorted: \", sample_sorted)\n",
    "            sort_idx = np.argsort(X[:, feature])\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\n",
    "            #print(\"y_sorted\", y_sorted)\n",
    "\n",
    "            sample_per_class_left = [0] * n_classes   #[0, 0]\n",
    "\n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\n",
    "\n",
    "            #loop through each threshold, 2.5, 6.5, 14.5\n",
    "            #1st iter: [-] [-++]\n",
    "            #2nd iter: [--] [++]\n",
    "            #3rd iter: [--+] [+]\n",
    "            for i in range(1, n_samples): #1 to 3 (excluding 4)\n",
    "                #the class of that sample\n",
    "                c = y_sorted[i - 1]  #[0]\n",
    "\n",
    "                #put the sample to the left\n",
    "                sample_per_class_left[c] += 1  #[1, 0]\n",
    "\n",
    "                #take the sample out from the right  [1, 2]\n",
    "                sample_per_class_right[c] -= 1\n",
    "                #print(sample_per_class_left)\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (sample_per_class_left[x] / i) ** 2 for x in range(n_classes)\n",
    "                )\n",
    "\n",
    "                #we divided by n_samples - i since we know that the left amount of samples\n",
    "                #since left side has already i samples\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(n_classes)\n",
    "                )\n",
    "\n",
    "                #weighted gini\n",
    "                weighted_gini = ((i / n_samples) * gini_left) + ( (n_samples - i) /n_samples) * gini_right\n",
    "\n",
    "                # in case the value are the same, we do not split\n",
    "                # (both have to end up on the same side of a split).\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    feature_ix = feature\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2  # midpoint\n",
    "\n",
    "        #return the feature number and threshold \n",
    "        #used to find best split\n",
    "        return feature_ix, threshold\n",
    "    \n",
    "    def fit(self, Xtrain, ytrain, n_classes, depth=0):  \n",
    "        n_samples, n_features = Xtrain.shape\n",
    "        num_samples_per_class = [np.sum(ytrain == i) for i in range(n_classes)]\n",
    "        #predicted class using the majority of sample class\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "\n",
    "        #define the parent node\n",
    "        node = Node(\n",
    "            gini = 1 - sum((np.sum(y == c) / n_samples) ** 2 for c in range(n_classes)),\n",
    "            predicted_class=predicted_class,\n",
    "            num_samples = ytrain.size,\n",
    "            num_samples_per_class = num_samples_per_class,\n",
    "            )\n",
    "        \n",
    "        if depth==0:\n",
    "            self.tree=node\n",
    "        if depth<self.max_depth:\n",
    "            #perform recursion\n",
    "            feature, threshold = self.find_split(Xtrain, ytrain, n_classes)\n",
    "            if feature is not None:\n",
    "                #take all the indices that is less than threshold\n",
    "                indices_left = X[:, feature] < threshold\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "\n",
    "                #tilde for negation\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "\n",
    "                #take note for later decision\n",
    "                node.feature_index = feature\n",
    "                node.threshold = threshold\n",
    "                node.left = self.fit(X_left, y_left, n_classes, depth + 1)\n",
    "                node.right = self.fit(X_right, y_right, n_classes, depth + 1)\n",
    "        return node\n",
    "    \n",
    "    def _predict(self, sample):\n",
    "        node = self.tree\n",
    "        #print(sample)\n",
    "        #print(node.feature_index)\n",
    "        while node.left:\n",
    "            if sample[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict(xrow) for xrow in X]\n",
    "    \n",
    "    def k_fold_cross_validation(self, X_train, y_train, depth_range, n_classes, k=10):\n",
    "        #implementing k fold cross validation for determining the best value of hyperparameter k=no. of neighbours\n",
    "        #We divide the training set into k parts. We withhold one part for testing and use the rest for training.\n",
    "        #Loop through all the folds, by keeping each one separate as a testing set and using the rest for training.\n",
    "        \n",
    "        fold_size = int(X_train.shape[0]/k)\n",
    "        accuracy = {}\n",
    "        for d in depth_range:\n",
    "            self.max_depth = d\n",
    "            accuracy[self.max_depth]=[]\n",
    "            for i in range(0,X_train.shape[0],fold_size):\n",
    "                xtest = X_train[i:i+fold_size]\n",
    "                ytest = y_train[i:i+fold_size]\n",
    "                xtrain = np.concatenate((X_train[:i], X_train[i+fold_size:]), axis=0)\n",
    "                ytrain = np.concatenate((y_train[:i], y_train[i+fold_size:]), axis=0)\n",
    "                self.fit(xtrain, ytrain, n_classes)\n",
    "                yhat = self.predict(xtest)\n",
    "                accuracy[self.max_depth].append( np.sum(yhat==ytest)/ytest.shape[0] )\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7abc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\n",
    "n_classes=len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e143ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for max_depth = 5:  0.8888888888888888\n",
      "accuracy for max_depth = 7:  0.9111111111111111\n",
      "accuracy for max_depth = 9:  0.8888888888888888\n",
      "accuracy for max_depth = 11:  0.8888888888888888\n",
      "accuracy for max_depth = 13:  0.9111111111111111\n",
      "accuracy for max_depth = 15:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "for d in range(5,16,2):\n",
    "    clf = dtree(max_depth=d)\n",
    "    clf.fit(X_train, y_train, n_classes)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy= np.sum(y_pred==y_test)/y_test.shape[0]\n",
    "    print(f\"accuracy for max_depth = {d}: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f9cb496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for k=5: 0.8952380952380953\n",
      "Mean accuracy for k=7: 0.8952380952380953\n",
      "Mean accuracy for k=9: 0.8857142857142858\n",
      "Mean accuracy for k=11: 0.8952380952380953\n",
      "Mean accuracy for k=12: 0.7238095238095239\n",
      "Mean accuracy for k=13: 0.8952380952380953\n",
      "Mean accuracy for k=14: 0.7333333333333334\n",
      "Mean accuracy for k=15: 0.8857142857142858\n",
      "The max accuracy achieved was 0.8952380952380953 for max_depth = 5.0\n"
     ]
    }
   ],
   "source": [
    "clf = dtree(max_depth=5)\n",
    "\n",
    "d_range = [5, 7, 9, 11, 12, 13, 14, 15]\n",
    "accuracy = clf.k_fold_cross_validation(X_train, y_train, d_range, n_classes, k=5 )\n",
    "acc=np.zeros((len(d_range),2))\n",
    "for idx, key in enumerate(d_range):\n",
    "    accuracy[key] = np.mean(accuracy[key])\n",
    "    print(f\"Mean accuracy for k={key}: {accuracy[key]}\")\n",
    "    acc[idx,0] = key\n",
    "    acc[idx,1] = accuracy[key]\n",
    "#print(acc)\n",
    "print(f\"The max accuracy achieved was {acc[acc.argmax(axis=0)[1],1]} for max_depth = {acc[acc.argmax(axis=0)[1],0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4feef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
