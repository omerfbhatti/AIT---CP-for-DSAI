{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1f9ce6",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - AdaBoost\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db19b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Muhammad Omer Farooq Bhatti\"\n",
    "Id = \"122498\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3879998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ccc9b",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "**AdaBoost** is a boosting algorithm that try to take a weak classifier on top of one another, **boosting** the overall performance. AdaBoost is extremely simple to use and implement, and often gives very effective results. There is tremendous flexibility in the choice of weak classifier as well. Anyhow, Decision Tree with <code>max_depth=1</code> and <code>max_leaf_nodes=2</code> are often used (also known as **stump**)\n",
    "\n",
    "<img src = \"../../Figures/ada.png\" />\n",
    "\n",
    "Suppose we are given training data ${(\\mathbf{x_i}, y_i)}$, where $\\mathbf{x_i} \\in \\mathbb{R}^n$ and $y_i \\in \\{-1, 1\\}$.  And we have $S$ number of weak classifiers, denoted $h_s(x)$.  For each classifier, we define $\\alpha_s$ as the *voting power* of the classifier $h_s(x)$. Then, the hypothesis function is based on a linear combination of the weak classifier and is written as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h(x) & = \\text{sign}\\big(\\alpha_1h_1(x) + \\alpha_2h_2(x) + \\cdots + \\alpha_sh_s(x) )\\big) \\\\\n",
    "& = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our job is to find the optimal $\\alpha_s$, so we can know which classifier we should give more weightage (i.e., believe more) in our hypothesis function since their accuracy is relatively better compared to other classifiers.  To get this alpha, we should define what is \"good\" classifier.  This is simple, since good classifier should simply has the minimum weighted errors as:\n",
    "\n",
    "$$\\epsilon_s = \\sum_{i=1}^m w_i^{s}I(h_s(x_i) \\neq y_i) $$\n",
    "\n",
    "in which the weights are initialized in the beginning as\n",
    "\n",
    "$$w_i^{(1)} = \\frac{1}{m}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea235e6",
   "metadata": {},
   "source": [
    "After we perform the first classifier, we update the weights of the samples using this formula:\n",
    "\n",
    "$$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\displaystyle\\sum_{i=1}^m w_i^{s}}} $$\n",
    "\n",
    "where $\\alpha_s$ is:\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bb96b",
   "metadata": {},
   "source": [
    "## Putting everything together:\n",
    "\n",
    "1. Loop through all features, threshold, and polarity, identify the best stump which has lowest weighted errors.\n",
    "\n",
    "2. Calculate alpha of the first classifier\n",
    "\n",
    "$$\\alpha_s = \\frac{1}{2}ln\\frac{1-\\epsilon_s}{\\epsilon_s}$$\n",
    "\n",
    "3. Exaggerate the incorrect samples using\n",
    "\n",
    "$$w_i^{(s+1)} = \\frac{w_i^{(s)}e^{ -\\alpha_sh_s(\\mathbf{x_i}) y_i}}{{\\displaystyle\\sum_{i=1}^m w_i^{s}}} $$\n",
    "\n",
    "4. Repeat 1.\n",
    "\n",
    "5. We stop 1-4 using max_iter, early stopping, or number of classifiers.\n",
    "\n",
    "6. To predict, we use the hypothesis function:\n",
    "\n",
    "$$ \n",
    "  H(x) = \\text{sign}\\big(\\sum_{s=1}^{S}\\alpha_sh_s(x)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc91913",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index\n",
    "- Put everything into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a377f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaptiveBoostClassifier:\n",
    "    def __init__(self, S=20):\n",
    "        self.S=S            #Number of classifiers\n",
    "        self.models=[]\n",
    "        model_parameters = {'max_depth':1, 'max_leaf_nodes':2}\n",
    "        for i in range(self.S):\n",
    "            self.models.append( DecisionTreeClassifier(**model_parameters) )  #Declaring models using **keyword args\n",
    "        \n",
    "        self.alpha = np.zeros(self.S) #alpha values for all classifiers\n",
    "       \n",
    "        \n",
    "    def fit(self, X, y, eta=1):              # <-- eta is the learning rate used to update alpha values\n",
    "        \n",
    "        sample_weights = np.full(X.shape[0], 1/X.shape[0]) #Weight vector of size (m= number of samples), \n",
    "                                                           #initialized to 1/m for each element\n",
    "        \n",
    "        for idx, model in enumerate(self.models):\n",
    "            model.fit(X, y, sample_weight=sample_weights) #For each classifier, train with sample_weights\n",
    "            yhat = model.predict(X)                       #Perform prediction using the training samples themselves\n",
    "            \n",
    "            error = sample_weights[yhat!=y].sum()  #Computing error 'Es' for the model based on summation of Weights \n",
    "                                                   #assigned to individual training samples\n",
    "            \n",
    "            #Higher 'Es' --> lower alpha (and vice versa)\n",
    "            self.alpha[idx] = eta*np.log((1-error)/error+0.001)   #Use 'Es' error to get \n",
    "            #print(f\"alpha[{idx}]\", self.alpha[idx])              #alpha (voting power for the classifier),\n",
    "                                                                #also used for re-evaluating weights for individual   \n",
    "                                                              #training samples, which are used to train the next model\n",
    "\n",
    "            #Update weights assigned to individual training samples\n",
    "            #These updated weights are used to train the next classifier \n",
    "            sample_weights = sample_weights * np.exp(-self.alpha[idx]* y * yhat)\n",
    "            sample_weights = sample_weights/np.sum(sample_weights)  #Normalize sample_weights\n",
    "            #print(f\"max weight: \", sample_weights[sample_weights.argmax()])\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        #Each classifier gets a corresponding weightage as per pre-calculated alpha\n",
    "        yhat=0\n",
    "        for idx, model in enumerate(self.models):\n",
    "            yhat = yhat + self.alpha[idx] * model.predict(X_test)\n",
    "        yhat = np.sign(yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ceb57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "#Make classification data\n",
    "X, y = make_classification(n_samples=500, random_state = 100)\n",
    "\n",
    "#Replace y=0 with y=-1 (same as with SVM)\n",
    "y[y==0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "model = adaptiveBoostClassifier(S=10)\n",
    "model.fit(X_train, y_train, eta=0.5)\n",
    "yhat = model.predict(X_test)\n",
    "print(\"Accuracy = \", np.sum(yhat==y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0255a6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Eta: 0.92, Accuracy =  0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "#Permuting different values of eta to see model's response\n",
    "learning_rates = np.linspace(0.01, 1.0, 100)     #range() sequence only works for integer values\n",
    "max_accuracy=0\n",
    "best_eta = 0\n",
    "for eta in learning_rates:              #loop through 100 values to find best Eta\n",
    "    model.fit(X_train, y_train, eta)\n",
    "    yhat = model.predict(X_test)\n",
    "    accuracy = np.sum(yhat==y_test)/y_test.shape[0]\n",
    "    if accuracy>max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        best_eta = eta\n",
    "    \n",
    "print(f\"Best Eta: {best_eta}, Accuracy = \", max_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2cfbd",
   "metadata": {},
   "source": [
    "1. Reference: https://engineering.purdue.edu/kak/Tutorials/AdaBoost.pdf\n",
    "2. https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3e71087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class stump:\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "        \n",
    "class adaptiveBoostwithStump:\n",
    "    def __init__(self, S=20):\n",
    "        self.S=S            #Number of classifiers\n",
    "        self.models=[]\n",
    "        for i in range(self.S):\n",
    "            self.models.append( stump() )  #Declaring models \n",
    "        \n",
    "        self.alpha = np.zeros(self.S) #alpha values for all classifiers\n",
    "       \n",
    "        \n",
    "    def fit(self, X, y, eta=0.5):              # <-- eta is the learning rate used to update alpha values\n",
    "        \n",
    "        sample_weights = np.full(X.shape[0], 1/X.shape[0]) #Weight vector of size (m= number of samples), \n",
    "                                                           #initialized to 1/m for each element\n",
    "        \n",
    "        for idx, model in enumerate(self.models):\n",
    "           \n",
    "            minimum_error= np.inf  #setting Minimum error to infinity for later evaluation\n",
    "            \n",
    "            subsample_idx = sample_weights.argsort()       #Get sorted weight indexes lowest to highest\n",
    "            subsample_idx = subsample_idx[:-int(len(subsample_idx)/4)]   # sampling for 1/4th of total samples\n",
    "            xt = X[subsample_idx]      #subsampling for training model based on large sample_weights\n",
    "            \n",
    "            for feature in range(X.shape[1]):    #Repeat for every feature in training set\n",
    "                \n",
    "                #Ordered list of samples for each feature\n",
    "                unique_feature_values_sorted = np.sort(np.unique(xt[:, feature])) #Choosing only samples with largest weights \n",
    "                \n",
    "                #  ( [1, 2, 3, 4] + [2, 3, 4, 5])/2 = [1.5, 2.5, 3.5, 4.5 ] <-- example to explain code line below\n",
    "                thresholds = (unique_feature_values_sorted[:-1] + unique_feature_values_sorted[1:])/2 \n",
    "                \n",
    "                for threshold in thresholds:        #Stepping through feature threshold values of our subsamples data\n",
    "                    for polarity in [1, -1]:           #checking for error for both polarities\n",
    "                        yhat = np.ones(X.shape[0]) #initialize prediction for all training samples to 1\n",
    "                        \n",
    "                        #X[features]<threshold --> y = -1 --> classify as -1 \n",
    "                        #-X[features] < -threshold --> y = -1 --> classify as -1\n",
    "                        yhat[ polarity * X[:, feature] < polarity * threshold ] = -1 #checking over all training samples\n",
    "                        \n",
    "                        #Misclassification Rate (Type 1 for polarity = 1 and Type 2 for polarity = -1)\n",
    "                        error = sample_weights[yhat!=y].sum()   #Adding weighted contribution made by each sample\n",
    "                        \n",
    "                        if error < minimum_error:\n",
    "                            minimum_error = error          #Defining model with minimum misclassification rate\n",
    "                            model.polarity = polarity      #Recording polarity for minimum error for later prediction\n",
    "                            model.threshold = threshold    #Recording threshold for minimum error for later prediction\n",
    "                            model.feature_index = feature  #Recording feature index for minimum error for later prediction\n",
    "            \n",
    "            #Set alpha = voting power for model\n",
    "            model.alpha = eta * np.log((1-minimum_error)/(minimum_error+0.00000001))\n",
    "            self.alpha[idx] = model.alpha\n",
    "            \n",
    "            #Update weights assigned to individual training samples\n",
    "            #These updated weights are used to train the next classifier \n",
    "            sample_weights = sample_weights * np.exp(-self.alpha[idx]* y * yhat)\n",
    "            sample_weights = sample_weights/np.sum(sample_weights)  #Normalize sample_weights\n",
    "            #print(f\"max weight: \", sample_weights[sample_weights.argmax()])\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        #Each classifier gets a corresponding weightage as per pre-calculated alpha\n",
    "        yhat=0\n",
    "        for model in self.models:\n",
    "            predictions = np.ones(X_test.shape[0])  #initialize to 1\n",
    "            #Predicting using our logic previously employed for calculating misclassification error\n",
    "            predictions[model.polarity * X_test[:, model.feature_index] < model.polarity * model.threshold] = -1\n",
    "            yhat = yhat + model.alpha * predictions\n",
    "        yhat = np.sign(yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "823e6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.68\n"
     ]
    }
   ],
   "source": [
    "#Make classification data\n",
    "X, y = make_classification(n_samples=500, random_state = 100)\n",
    "\n",
    "#Replace y=0 with y=-1 (same as with SVM)\n",
    "y[y==0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "model = adaptiveBoostwithStump(S=10)\n",
    "model.fit(X_train, y_train, eta=0.5)\n",
    "yhat = model.predict(X_test)\n",
    "print(\"Accuracy = \", np.sum(yhat==y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1349e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: 1, Eta: 0.1, Accuracy =  0.8933333333333333\n",
      "S: 1, Eta: 0.325, Accuracy =  0.8933333333333333\n",
      "S: 1, Eta: 0.55, Accuracy =  0.8933333333333333\n",
      "S: 1, Eta: 0.775, Accuracy =  0.8933333333333333\n",
      "S: 1, Eta: 1.0, Accuracy =  0.8933333333333333\n",
      "S: 3, Eta: 0.1, Accuracy =  0.9\n",
      "S: 3, Eta: 0.325, Accuracy =  0.8733333333333333\n",
      "S: 3, Eta: 0.55, Accuracy =  0.68\n",
      "S: 3, Eta: 0.775, Accuracy =  0.68\n",
      "S: 3, Eta: 1.0, Accuracy =  0.68\n",
      "S: 5, Eta: 0.1, Accuracy =  0.9066666666666666\n",
      "S: 5, Eta: 0.325, Accuracy =  0.68\n",
      "S: 5, Eta: 0.55, Accuracy =  0.68\n",
      "S: 5, Eta: 0.775, Accuracy =  0.68\n",
      "S: 5, Eta: 1.0, Accuracy =  0.68\n",
      "S: 6, Eta: 0.1, Accuracy =  0.8866666666666667\n",
      "S: 6, Eta: 0.325, Accuracy =  0.68\n",
      "S: 6, Eta: 0.55, Accuracy =  0.68\n",
      "S: 6, Eta: 0.775, Accuracy =  0.68\n",
      "S: 6, Eta: 1.0, Accuracy =  0.68\n",
      "S: 8, Eta: 0.1, Accuracy =  0.8733333333333333\n",
      "S: 8, Eta: 0.325, Accuracy =  0.68\n",
      "S: 8, Eta: 0.55, Accuracy =  0.68\n",
      "S: 8, Eta: 0.775, Accuracy =  0.68\n",
      "S: 8, Eta: 1.0, Accuracy =  0.68\n",
      "S: 10, Eta: 0.1, Accuracy =  0.68\n",
      "S: 10, Eta: 0.325, Accuracy =  0.68\n",
      "S: 10, Eta: 0.55, Accuracy =  0.68\n",
      "S: 10, Eta: 0.775, Accuracy =  0.68\n",
      "S: 10, Eta: 1.0, Accuracy =  0.68\n",
      "S: 13, Eta: 0.1, Accuracy =  0.68\n",
      "S: 13, Eta: 0.325, Accuracy =  0.68\n",
      "S: 13, Eta: 0.55, Accuracy =  0.68\n",
      "S: 13, Eta: 0.775, Accuracy =  0.68\n",
      "S: 13, Eta: 1.0, Accuracy =  0.68\n",
      "S: 15, Eta: 0.1, Accuracy =  0.68\n",
      "S: 15, Eta: 0.325, Accuracy =  0.68\n",
      "S: 15, Eta: 0.55, Accuracy =  0.68\n",
      "S: 15, Eta: 0.775, Accuracy =  0.68\n",
      "S: 15, Eta: 1.0, Accuracy =  0.68\n",
      "S: 17, Eta: 0.1, Accuracy =  0.68\n",
      "S: 17, Eta: 0.325, Accuracy =  0.68\n",
      "S: 17, Eta: 0.55, Accuracy =  0.68\n",
      "S: 17, Eta: 0.775, Accuracy =  0.68\n",
      "S: 17, Eta: 1.0, Accuracy =  0.68\n",
      "S: 20, Eta: 0.1, Accuracy =  0.68\n",
      "S: 20, Eta: 0.325, Accuracy =  0.68\n",
      "S: 20, Eta: 0.55, Accuracy =  0.68\n",
      "S: 20, Eta: 0.775, Accuracy =  0.68\n",
      "S: 20, Eta: 1.0, Accuracy =  0.68\n"
     ]
    }
   ],
   "source": [
    "#Permuting different values of S & eta to see model's response\n",
    "learning_rates = np.linspace(0.1, 1.0, 5)     #range() sequence only works for integer values\n",
    "max_accuracy=0\n",
    "best_eta = 0\n",
    "for s in [1, 3, 5, 6, 8, 10, 13, 15, 17, 20]:\n",
    "    model = adaptiveBoostwithStump(S=s)\n",
    "    for eta in learning_rates:              #loop through 100 values to find best Eta\n",
    "        model.fit(X_train, y_train, eta)\n",
    "        yhat = model.predict(X_test)\n",
    "        accuracy = np.sum(yhat==y_test)/y_test.shape[0]\n",
    "        print(f\"S: {s}, Eta: {eta}, Accuracy = \", accuracy)\n",
    "\n",
    "        ## HIGHEST ACCURACY ACHIEVED AT S=5, eta=0.1, accuracy = 0.9066666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48faa2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.74\n"
     ]
    }
   ],
   "source": [
    "#Make classification data with 3 informative features\n",
    "X, y = make_classification(n_samples=500, n_informative=3, random_state = 100)\n",
    "\n",
    "#Replace y=0 with y=-1 (same as with SVM)\n",
    "y[y==0] = -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "model = adaptiveBoostwithStump(S=10)\n",
    "model.fit(X_train, y_train, eta=0.5)\n",
    "yhat = model.predict(X_test)\n",
    "print(\"Accuracy = \", np.sum(yhat==y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b6d6606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: 1, Eta: 0.1, Accuracy =  0.7933333333333333\n",
      "S: 1, Eta: 0.325, Accuracy =  0.7933333333333333\n",
      "S: 1, Eta: 0.55, Accuracy =  0.7933333333333333\n",
      "S: 1, Eta: 0.775, Accuracy =  0.7933333333333333\n",
      "S: 1, Eta: 1.0, Accuracy =  0.7933333333333333\n",
      "S: 3, Eta: 0.1, Accuracy =  0.7933333333333333\n",
      "S: 3, Eta: 0.325, Accuracy =  0.7933333333333333\n",
      "S: 3, Eta: 0.55, Accuracy =  0.74\n",
      "S: 3, Eta: 0.775, Accuracy =  0.74\n",
      "S: 3, Eta: 1.0, Accuracy =  0.74\n",
      "S: 5, Eta: 0.1, Accuracy =  0.7933333333333333\n",
      "S: 5, Eta: 0.325, Accuracy =  0.74\n",
      "S: 5, Eta: 0.55, Accuracy =  0.74\n",
      "S: 5, Eta: 0.775, Accuracy =  0.74\n",
      "S: 5, Eta: 1.0, Accuracy =  0.74\n",
      "S: 6, Eta: 0.1, Accuracy =  0.7933333333333333\n",
      "S: 6, Eta: 0.325, Accuracy =  0.74\n",
      "S: 6, Eta: 0.55, Accuracy =  0.74\n",
      "S: 6, Eta: 0.775, Accuracy =  0.74\n",
      "S: 6, Eta: 1.0, Accuracy =  0.74\n",
      "S: 8, Eta: 0.1, Accuracy =  0.7866666666666666\n",
      "S: 8, Eta: 0.325, Accuracy =  0.74\n",
      "S: 8, Eta: 0.55, Accuracy =  0.74\n",
      "S: 8, Eta: 0.775, Accuracy =  0.74\n",
      "S: 8, Eta: 1.0, Accuracy =  0.74\n",
      "S: 10, Eta: 0.1, Accuracy =  0.78\n",
      "S: 10, Eta: 0.325, Accuracy =  0.74\n",
      "S: 10, Eta: 0.55, Accuracy =  0.74\n",
      "S: 10, Eta: 0.775, Accuracy =  0.74\n",
      "S: 10, Eta: 1.0, Accuracy =  0.74\n",
      "S: 13, Eta: 0.1, Accuracy =  0.74\n",
      "S: 13, Eta: 0.325, Accuracy =  0.74\n",
      "S: 13, Eta: 0.55, Accuracy =  0.74\n",
      "S: 13, Eta: 0.775, Accuracy =  0.74\n",
      "S: 13, Eta: 1.0, Accuracy =  0.74\n",
      "S: 15, Eta: 0.1, Accuracy =  0.74\n",
      "S: 15, Eta: 0.325, Accuracy =  0.74\n",
      "S: 15, Eta: 0.55, Accuracy =  0.74\n",
      "S: 15, Eta: 0.775, Accuracy =  0.74\n",
      "S: 15, Eta: 1.0, Accuracy =  0.74\n",
      "S: 17, Eta: 0.1, Accuracy =  0.74\n",
      "S: 17, Eta: 0.325, Accuracy =  0.74\n",
      "S: 17, Eta: 0.55, Accuracy =  0.74\n",
      "S: 17, Eta: 0.775, Accuracy =  0.74\n",
      "S: 17, Eta: 1.0, Accuracy =  0.74\n",
      "S: 20, Eta: 0.1, Accuracy =  0.74\n",
      "S: 20, Eta: 0.325, Accuracy =  0.74\n",
      "S: 20, Eta: 0.55, Accuracy =  0.74\n",
      "S: 20, Eta: 0.775, Accuracy =  0.74\n",
      "S: 20, Eta: 1.0, Accuracy =  0.74\n"
     ]
    }
   ],
   "source": [
    "#Permuting different values of S & eta to see model's response\n",
    "learning_rates = np.linspace(0.1, 1.0, 5)     #range() sequence only works for integer values\n",
    "max_accuracy=0\n",
    "best_eta = 0\n",
    "for s in [1, 3, 5, 6, 8, 10, 13, 15, 17, 20]:\n",
    "    model = adaptiveBoostwithStump(S=s)\n",
    "    for eta in learning_rates:              #loop through 100 values to find best Eta\n",
    "        model.fit(X_train, y_train, eta)\n",
    "        yhat = model.predict(X_test)\n",
    "        accuracy = np.sum(yhat==y_test)/y_test.shape[0]\n",
    "        print(f\"S: {s}, Eta: {eta}, Accuracy = \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
