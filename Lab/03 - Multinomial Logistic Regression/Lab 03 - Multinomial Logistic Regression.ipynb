{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2cb0fd",
   "metadata": {},
   "source": [
    "Lab 03 - Multinomial Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168985b4",
   "metadata": {},
   "source": [
    "$Name = \"Muhammad Omer Farooq Bhatti\"$\n",
    "$Id = \"st122498\"$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed7226",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression\n",
    "\n",
    "This is logistic regression when number of classes are more than 2.\n",
    "\n",
    "### Scratch\n",
    "\n",
    "**Implementation steps:**\n",
    "    \n",
    "The gradient descent has the following steps:\n",
    "\n",
    "1. Prepare your data\n",
    "    - add intercept\n",
    "    - $\\mathbf{X}$ and $\\mathbf{Y}$ and $\\mathbf{W}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{Y}$ -> $(m, k)$\n",
    "        - $\\mathbf{W}$ -> $(n, k)$\n",
    "        - where $k$ is number of classes\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Predict using the softmax function\n",
    "   $$ h = \\mathsf{P}(y = a \\mid \\theta) = \\frac{e^{\\theta^{T}_ax}}{\\Sigma_{i=1}^{k} e^{\\theta_k^{T}x}}$$\n",
    "   --->why this function?<----\n",
    "   - First, mathematically, this is just an extension of the sigmoid formula for multi-class classification\n",
    "   - $e$ will always give non-negative outputs which helps, since probability is never negative\n",
    "   - $e$ has a similar effect as argmax, which will turn larger input to larger outputs.\n",
    "   - $e$ is super easy to differentiate, because derivative of $e$ is $e$\n",
    "   - $e$ nicely cancel out the $\\log$ in the cross entropy loss (see below)\n",
    "   - By dividing, it make sure all the probability adds up to one.  You can think the softmax function as some form of normalization.   Why not normalization?  Because normalization cares only about proportion, while softmax reacts to change in scale better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558258a",
   "metadata": {},
   "source": [
    "3. Calculate the loss using the cross entropy loss\n",
    "    $$J = -\\sum_{i=1}^m y^{(i)}\\log(h^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c751ca3",
   "metadata": {},
   "source": [
    "4. Calculate the gradient of theta of feature $j$ based on the loss function $J$\n",
    "    - The gradient is defined as\n",
    "       $$\\frac{\\partial J}{\\partial \\theta_j} = \\sum_{i=1}^{m}(h^{(i)}-y^{(i)})x_j$$\n",
    "    - This gradient can be derived from the following simple example:\n",
    "        - Suppose given 2 classes (k = 2) and 3 features (n = 3), we have the loss function as\n",
    "       $$ J = -y_1 \\log h_1 - y_2 \\log h_2 $$\n",
    "       where $h_1$ and $h_2$ are\n",
    "       $$ h_1 = \\frac{\\exp(g_1)}{\\exp(g_1)+\\exp(g_2)} $$\n",
    "       $$ h_2 = \\frac{\\exp(g_2)}{\\exp(g_1)+\\exp(g_2)} $$\n",
    "       where $g_1$ and $g_2$ are\n",
    "       $$ g_1 = w_{11}x_1 + w_{21}x_2 + w_{31}x_3 $$\n",
    "       $$ g_2 = w_{12}x_1 + w_{22}x_2 + w_{32}x_3  $$\n",
    "       where in $w_{ij}$, $i$ stands for feature and $j$ stands for class \n",
    "    - For example, to find the gradient of $J$ in respect to $w_{21}$, we simply can use the chain rule (or backpropagation) to calculate like this:\n",
    "       $$ \\frac{\\partial J}{\\partial w_{21}} = \\frac{\\partial J}{\\partial h_{1}}\\frac{\\partial h_{1}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}} + \\frac{\\partial J}{\\partial h_{2}}\\frac{\\partial h_{2}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}}$$\n",
    "   - If we know each of them, it is easy, where\n",
    "       $$\\frac{\\partial J}{\\partial h_{1}} = -\\frac{y_1}{h_1}$$\n",
    "       $$\\frac{\\partial J}{\\partial h_{2}} = -\\frac{y_2}{h_2}$$\n",
    "       $$\\frac{\\partial h_{1}}{\\partial g_{1}} = \\frac{\\exp(g_{1})}{\\exp(g_{1}) + \\exp(g_{2})} - (\\frac{\\exp(g_1)}{\\exp(g_1)+\\exp(g_2)})^2 = h_1 (1 - h_1)$$\n",
    "       $$\\frac{\\partial h_{2}}{\\partial g_{1}} = \\frac{-exp(g_2)exp(g_1)}{(\\exp(g_1) + \\exp(g_2)^2} = -h_2h_1$$\n",
    "       $$\\frac{\\partial g_1}{\\partial w_{21}} = x_2$$\n",
    "    - For those who forgets how to do third and fourth, recall that the quotient rule\n",
    "        $$ (\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$$\n",
    "    - Putting everything together, we got\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial w_{21}} &= \\frac{\\partial J}{\\partial h_{1}}\\frac{\\partial h_{1}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}} + \\frac{\\partial J}{\\partial h_{2}}\\frac{\\partial h_{2}}{\\partial g_{1}}\\frac{\\partial g_{1}}{\\partial w_{21}}\\\\\n",
    "&= -\\frac{y_1}{h_1} * h_1 (1 - h_1) * x_2 + -\\frac{y_2}{h_2} * -h_2h_1 * x_2 \\\\\n",
    "&= x_2 (-y_1 + y_1h_1 + y_2h_1)\\\\\n",
    "&= x_2 (-y_1 + h_1(y_1 + y_2))\\\\\n",
    "&= x_2 (h_1 - y_1)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "4. Update the theta with this update rule\n",
    "    $$\\theta_j := \\theta_j - \\alpha * \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "    where $\\alpha$ is a typical learning rate range between 0 and 1\n",
    "5. Loop 2-4 until max_iter is reached, or the difference between old loss and new loss are smaller than some predefined threshold tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e8c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707f8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (2, 3)\n",
      "Y:  (2, 4)\n",
      "W:  (3, 4)\n",
      "X @ W: [[ 8 14 20  7]\n",
      " [15 26 35 13]]\n",
      "softmax(X @ W): [[6.12896865e-06 2.47260243e-03 9.97519014e-01 2.25472156e-06]\n",
      " [2.06089928e-09 1.23394576e-04 9.99876603e-01 2.78912388e-10]]\n",
      "Try to confirm it adds up to 1: [1. 1.]\n",
      "if I want to know which one is the answer, use argmax:  [2 2]\n",
      "normalization(X @ W): [[0.30044631 0.52578104 0.75111577 0.26289052]\n",
      " [0.31311215 0.54272772 0.73059501 0.27136386]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "X = np.array([[1, 2, 3],\n",
    "             [2, 4, 5]])\n",
    "\n",
    "print(\"X: \", X.shape)  #(m, n) two samples, three features.  We ignore the y-intercept\n",
    "\n",
    "#m=2 , n=3\n",
    "\n",
    "Y = np.array([[0, 0, 1, 0],\n",
    "              [1, 0, 0, 0]])  #(m, k) let's say four classes\n",
    "#m=2 , k=4 \n",
    "print(\"Y: \", Y.shape)\n",
    "\n",
    "W = np.array([[1, 2, 3, 4],\n",
    "              [2, 3, 1, 0],\n",
    "              [1, 2, 5, 1],\n",
    "              ])  #(n, k)  three features, four classes\n",
    "\n",
    "print(\"W: \", W.shape)\n",
    "\n",
    "print(\"X @ W:\",  X @ W)  #X @ W should be the same shape as our y\n",
    "\n",
    "print(\"softmax(X @ W):\", softmax(X@W))\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(\"Try to confirm it adds up to 1:\", softmax(X@W).sum(axis=1))\n",
    "\n",
    "print(\"if I want to know which one is the answer, use argmax: \", np.argmax(softmax(X@W), axis=1))\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "print(\"normalization(X @ W):\", normalize(X@W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08aba9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:  [[0 0 1 0]\n",
      " [1 0 0 0]]\n",
      "h:  [[0.00000613 0.0024726  0.99751901 0.00000225]\n",
      " [0.         0.00012339 0.9998766  0.        ]]\n",
      "log:  [[ -0.          -0.          -0.00248407  -0.        ]\n",
      " [-20.0001234   -0.          -0.          -0.        ]]\n",
      "log loss:  [[ 0.          0.          0.00248407  0.        ]\n",
      " [20.0001234   0.          0.          0.        ]]\n",
      "sum of log loss:  20.00260747339262\n"
     ]
    }
   ],
   "source": [
    "#calculate the loss\n",
    "print(\"Y: \", Y)\n",
    "print(\"h: \", softmax(X@W))\n",
    "print(\"log: \", Y * np.log(softmax(X@W)))\n",
    "print(\"log loss: \", -(Y * np.log(softmax(X@W))))\n",
    "print(\"sum of log loss: \", np.sum(-(Y * np.log(softmax(X@W)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98973b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879b7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7442a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression:\n",
    "    def __init__(self, X, y, k, l_rate = 0.001):\n",
    "        self.X_train = X\n",
    "        self.m = self.X_train.shape[0]  # no.of samples\n",
    "        self.n = self.X_train.shape[1]  # no. of features\n",
    "        self.k=k\n",
    "        Y_train_encoded = np.zeros((self.m, self.k))\n",
    "        self.time_taken=0\n",
    "                                   \n",
    "        for n_class in range(self.k):\n",
    "            cond = y==n_class\n",
    "            Y_train_encoded[np.where(cond), n_class] = 1\n",
    "        self.Y_train = Y_train_encoded      #One hot encoded Y_training data\n",
    "        \n",
    "        self.W = []\n",
    "        self.alpha=l_rate\n",
    "        self.method='mini-batch'\n",
    "        \n",
    "    def fit(self, max_iter=10000, method='mini-batch' ):\n",
    "        self.method = method\n",
    "        plt.figure(figsize=(8,5))\n",
    "        \n",
    "        self.W, i = self.logistic_regression_GD(self.X_train, self.Y_train, \n",
    "                                        self.k, self.X_train.shape[1], max_iter)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def logistic_regression_GD(self, X, Y, k, n, max_iter):\n",
    "        '''\n",
    "        Inputs: \n",
    "            X shape: (m, n)\n",
    "            w shape: (n, k)\n",
    "        '''\n",
    "        start_time = time()\n",
    "        W = np.random.rand(n, k)\n",
    "        J_iter=np.zeros(max_iter)\n",
    "        \n",
    "        if (self.method == \"batch\"):\n",
    "            #Batch Gradient Descent\n",
    "            xtrain = X\n",
    "            ytrain = Y\n",
    "            \n",
    "            for i in range(max_iter):\n",
    "                #Predict h_theta and gradient using initial value of W\n",
    "                cost, grad =  self.gradient(xtrain, ytrain, W)\n",
    "                J_iter[i]= cost\n",
    "                #if i % 500 == 0:\n",
    "                #    J_iter[i]= cost\n",
    "                #    print(f\"Cost at iteration {i}\", cost)\n",
    "                #Update value of W\n",
    "                W = W - self.alpha * grad\n",
    "        \n",
    "        elif (self.method == \"mini-batch\"):\n",
    "            #Mini-Batch Gradient Descent\n",
    "            batch_size = int(0.1*(X.shape[0]))\n",
    "            for i in range(max_iter):\n",
    "                xtrain, ytrain = self.get_minibatch(batch_size, X, Y)\n",
    "                #Predict h_theta and gradient using initial value of W\n",
    "                cost, grad =  self.gradient(xtrain, ytrain, W)\n",
    "                J_iter[i]= cost\n",
    "                #if i % 500 == 0:\n",
    "                #    J_iter[i]= cost\n",
    "                #    print(f\"Cost at iteration {i}\", cost)\n",
    "                #Update value of W\n",
    "                W = W - self.alpha * grad\n",
    "                \n",
    "        elif (self.method == \"sto\"):\n",
    "            #Stochastic Gradient Descent\n",
    "            used_indexes=[]\n",
    "            for i in range(max_iter):\n",
    "                \n",
    "                xtrain, ytrain, used_indexes = self.get_data_stochasticGD(X, Y, used_indexes)\n",
    "                \n",
    "                #Predict h_theta and gradient using initial value of W\n",
    "                cost, grad =  self.gradient(xtrain, ytrain, W)\n",
    "                J_iter[i]= cost\n",
    "                #if i % 500 == 0:\n",
    "                #    J_iter[i]= cost\n",
    "                #    print(f\"Cost at iteration {i}\", cost)\n",
    "                #Update value of W\n",
    "                W = W - self.alpha * grad\n",
    "        else:\n",
    "            raise ValueError(f\"The value given for method argument (method = {self.method}) is not valid.\")\n",
    "        \n",
    "        self.time_taken = time()-start_time\n",
    "        self.plot_J_iter(J_iter,i)\n",
    "        return W, i\n",
    "\n",
    "    def get_minibatch(self, batch_size, X, y):\n",
    "        index=np.random.randint(0,X.shape[0]) #choose a contiguous block of samples \n",
    "        xtrain=X[index:index+batch_size]      #of sizs = batch_size at random\n",
    "        ytrain=y[index:index+batch_size]\n",
    "        return xtrain, ytrain\n",
    "    \n",
    "    def get_data_stochasticGD(self, X, Y, used_indexes):\n",
    "        n=np.random.randint(0, X.shape[0])\n",
    "        while n in used_indexes:\n",
    "            n=np.random.randint(0, X.shape[0])\n",
    "        used_indexes.append(n)\n",
    "        if ( len(used_indexes) == X.shape[0] ):\n",
    "            used_indexes=[]\n",
    "        xtrain=X[n,:].reshape(1,-1)\n",
    "        ytrain=np.array([Y[n]])\n",
    "        return xtrain, ytrain,  used_indexes\n",
    "                \n",
    "        \n",
    "    def plot_J_iter(self, J_iter, n_iter):\n",
    "        #plt.figure(figsize=(10,10))\n",
    "        plt.plot(np.arange(1,n_iter,1), J_iter[0:n_iter-1])\n",
    "        plt.xlabel(\"Number of iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"{self.method} Gradient Descent - Loss vs Iterations Graph\")\n",
    "        \n",
    "    def gradient(self, X, Y, W):\n",
    "        m = X.shape[0]\n",
    "        h = self.h_theta(X, W)\n",
    "        cost = - np.sum(Y * np.log(h)) / m\n",
    "        error = h - Y\n",
    "        grad = self.softmax_grad(X, error)\n",
    "        return cost, grad\n",
    "    \n",
    "    def softmax(self, theta_t_x):\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_grad(self, X, error):\n",
    "        return  X.T @ error\n",
    "        \n",
    "    def h_theta(self, X, W):\n",
    "        '''\n",
    "        Input:\n",
    "            X shape: (m, n)\n",
    "            w shape: (n, k)\n",
    "        Returns:\n",
    "            yhat shape: (m, k)\n",
    "        '''\n",
    "        return self.softmax(X @ W)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        h_test = self.h_theta(X_test, self.W)\n",
    "        yhat = np.argmax(h_test, axis=1)\n",
    "        return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Prepare data\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, 2:]  # we only take the first two features.  #n=2\n",
    "y = iris.target  #now our y is three classes thus require multinomial   #k=3 \n",
    "\n",
    "# feature scaling helps improve reach convergence faster\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# add intercept to our X\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)  #add intercept\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)  #add intercept\n",
    "\n",
    "# make sure our y is in the shape of (m, k)\n",
    "# we will convert our output vector in \n",
    "# matrix where no. of columns is equal to the no. of classes. \n",
    "# The values in the matrix will be 0 or 1. For instance the rows \n",
    "# where we have output 2 the column 2 will contain 1 and the rest are all 0.\n",
    "# in simple words, y will be of shape (m, k)\n",
    "  # no. of class  (can also use np.unique)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1e0f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25122143 0.00087755]]\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "n=np.random.randint(0,X.shape[0])\n",
    "\n",
    "xtrain=X[n,:].reshape(1,-1)\n",
    "print(xtrain)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8785ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6512/1081408469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclassification_reports\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmethods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"mini-batch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sto\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=========Classification report=======\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "model = logisticRegression(X_train, y_train, k = len(set(y)))\n",
    "classification_reports = {}\n",
    "time_taken = {}\n",
    "methods = [\"batch\", \"mini-batch\", \"sto\"]\n",
    "print(\"=========Classification report=======\\n\")\n",
    "for GD_method in methods:\n",
    "    model.fit( max_iter = 4000, method = GD_method)\n",
    "    yhat = model.predict(X_test)\n",
    "    time_taken[GD_method] = model.time_taken\n",
    "    classification_reports[GD_method] =  classification_report(y_test, yhat)\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    print(f\"Time taken to run the {GD_method} algorithm is {time_taken[GD_method]} seconds.\\n\")\n",
    "    print(classification_reports[GD_method])\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    \n",
    "\n",
    "#print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5b3e482",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The value given for method argument (method = htr) is not valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/3332218139.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"htr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/1162494708.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, max_iter, method)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         self.W, i = self.logistic_regression_GD(self.X_train, self.Y_train, \n\u001b[0m\u001b[0;32m     23\u001b[0m                                         self.k, self.X_train.shape[1], max_iter)\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14240/1162494708.py\u001b[0m in \u001b[0;36mlogistic_regression_GD\u001b[1;34m(self, X, Y, k, n, max_iter)\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The value given for method argument (method = {self.method}) is not valid.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_J_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The value given for method argument (method = htr) is not valid."
     ]
    }
   ],
   "source": [
    "model = logisticRegression(X_train, y_train, k = len(set(y)))\n",
    "model.fit( method = \"htr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6f064e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "model = LogisticRegression(multi_class=\"ovr\")  #set this to multiclass=\"ovr\" to perform multinomial logistic\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "85a58638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========Classification report=======\n",
      "Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        17\n",
      "           1       0.93      0.81      0.87        16\n",
      "           2       0.79      0.92      0.85        12\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.90      0.91      0.90        45\n",
      "weighted avg       0.92      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=========Classification report=======\")\n",
    "print(\"Report: \", classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50dbb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
