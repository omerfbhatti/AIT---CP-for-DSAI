{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31afc947",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Classification - Gradient Boosting\n",
    "\n",
    "### Readings:\n",
    "- [GERON] Ch7\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch16\n",
    "- https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cee0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"Muhammad Omer Farooq Bhatti\"\n",
    "Id = \"122498\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7603fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston, load_breast_cancer, load_digits    # <---- Import dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier   # <---- Dummy Regressor to use as base model\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier   # <---- Decision Tree Regressor & Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ad362",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Another popular one is Gradient Boosting.  Similar to AdaBoost, Gradient Boosting works by adding sequential predictors.  However, instead of adding **weights**, this method tries to fit the new predictor to the **residual errors** made by the previous predictor.    The hypothesis function of gradient boosting is as follows:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "Although they look similar, notice that no alpha is applied to the first predictor.  In addition, each alpha is the same, as opposed to voting power in AdaBoost.  Typically, similar to AdaBoost, decision trees are used for each $h_i(x)$ but are not limited to stump.  In practice, min_leaves are set to around 8 to 32.\n",
    "\n",
    "Since gradient boosting actually originate from additive linear regression, we shall first talk about **gradient boosting for regression**.  Also assume that we are using **regression trees** for our regressors.\n",
    "\n",
    "### Gradient Boosting for Regression\n",
    "\n",
    "Firstly, let's look at the following equation where $h_0(x)$ is our first predictor and we would like to minimize the residual as follows:\n",
    "\n",
    "$$h_0(x) + residual_0 = y $$\n",
    "$$ residual_0 =  y - h_0(x) $$\n",
    "\n",
    "That is, we would $y$ to be as close as $h_0(x)$ such that residual is 0\n",
    "\n",
    "$$ y = h_0(x) $$\n",
    "\n",
    "The question is that is it possible to add the second predictor $h_1(x)$ such that the residual is further reduced\n",
    "\n",
    "$$ y = h_0(x) + h_1(x) $$\n",
    "\n",
    "This equation can be written as:\n",
    "\n",
    "$$h_1(x) = y - h_0(x) $$\n",
    "\n",
    "This equation informs us that if we can find a subsequent predictor that can best fit the \"residual\" (i.e. $y - h_0(x)$), then we can improve the accuracy.\n",
    "\n",
    "**How is this related to gradient descent?**\n",
    "\n",
    "Well, firstly, here is our loss function for regression:\n",
    "\n",
    "$$J = \\frac{1}{2}(y - h(x))^2$$\n",
    "\n",
    "And here, we want to minimize $J$ by gradient of the loss function in respect to by adjusting $h_x$.  We can thus treat $h_x$ as parameters and take derivatives:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = h(x) - y $$\n",
    "\n",
    "Thus, we can interpret residuals as negative gradients:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "y & = h_0(x) + h_1(x)\\\\\n",
    "& = h_0(x) + (y - h_0(x)) \\\\\n",
    "& = h_0(x) - (h_0(x) - y) \\\\\n",
    "& = h_0(x) - \\frac{\\partial J}{\\partial h_0(x)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So in fact, we are using \"gradient\" descent in \"gradient\" boosting to find the new model, written as:\n",
    "\n",
    "$$h_1(x) = - \\frac{\\partial J}{\\partial h_0(x)} = y - h_0(x)$$\n",
    "\n",
    "or more generally\n",
    "\n",
    "$$h_s(x) = - \\frac{\\partial J}{\\partial h_{s-1}(x)} = y - h_{s-1}(x)$$\n",
    "\n",
    "where $s$ is the index of predictor\n",
    "\n",
    "**So residuals or gradients?**\n",
    "\n",
    "Although they are equivalent in the mse loss function, it is more useful to use negative gradients as it is more general, and can apply to other loss functions as well, e.g., **cross-entropy** in the case of classification.\n",
    "\n",
    "In cross entropy, the loss function is\n",
    "\n",
    " $$J= y log h(x) + (1 - y) lg (1-h(x))$$\n",
    " \n",
    "If you look at our previous lecture on logistic regression, the derivative of this **in respect to h(x)** will be:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h_(x)} = h(x) - y$$\n",
    "\n",
    "This may look the same as mse, but note that our $h(x)$ (i.e., regression tree) outputs continuous values.  In order to transform $h(x)$ into discrete class, we shall transform using sigmoid function $g$ as follows:\n",
    "\n",
    "$$g(h(x)) = g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "For multiclass classification, $g$ is defined as the softmax function:\n",
    "\n",
    "$$g(h(x)) = g(z) = \\frac{e^z_c}{\\Sigma_{i=1}^{k} e^z_k}$$\n",
    "\n",
    "Also remind that to use softmax function, we need to first one-hot encode our y.  And during prediction, we need to perform <code>np.argmax</code> along the axis=1\n",
    "\n",
    "\n",
    "**Adding learning rate**\n",
    "\n",
    "To make sure adding the subsequent predictor would not overfit our model, we shall add an learning rate $\\alpha$ in front of this, which shall be the same across all predictors (different from AdaBoost where alpha is different across all predictors)\n",
    "\n",
    "$$h_s(x) = - \\alpha \\frac{\\partial J}{\\partial h_{s-1}(x)}$$\n",
    "\n",
    "**What about next predictor**\n",
    "\n",
    "We can stop if we are happy, either using some predefined iterations, or whether the residual does not decrease further using some validation set.  \n",
    "\n",
    "In this case, it is obvious that 2 predictors are simply not enough.  Thus, we first need to calculate the residuals which are\n",
    "\n",
    "$$ residual_1 =  y - (h_0(x) + \\alpha h_1(x))$$\n",
    "\n",
    "then we define $h_2(x)$ as \n",
    "\n",
    "$$h_2(x) = \\alpha(y - (h_0(x) + \\alpha h_1(x)))$$\n",
    "\n",
    "And then repeat\n",
    "\n",
    "The final prediction shall use the following hypothesis function $H(x)$:\n",
    "\n",
    "$$\n",
    "H(x) = h_0(x) + \\alpha_1h_1(x) + \\cdots + \\alpha_sh_s(x)\n",
    "$$\n",
    "\n",
    "**Summary of steps**\n",
    "\n",
    "1. Initialize the model as simply mean or some constant\n",
    "2. Predict and calculate the residual\n",
    "3. Let the next model fit the residual\n",
    "4. Predict using the combined models and calculate the residual\n",
    "5. Let the next model fit this residual\n",
    "6. Simply repeat 4-5 until stopping criteria is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e614513",
   "metadata": {},
   "source": [
    "### When to use Boosting\n",
    "\n",
    "Let's summarize some useful info about Gradient Boosting:\n",
    "\n",
    "Advantages:\n",
    "1. Extremely powerful - especially useful for heterogeneous data (e.g., house price, number of bedrooms). \n",
    "\n",
    "Disadvantages:\n",
    "1. They cannot be parallelized.  Obvious since they are sequential predictors.\n",
    "2. They can easily overfit, thus require careful choice of estimators or the use of regularization such as max_depth.\n",
    "3. When we talk about homogeneous data such as images, videos, audio, text, or huge amount of data, deep learning works better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1909935",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Modify the above scratch code such that:\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\n",
    "- Put everything into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f492edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gradientBoosting:\n",
    "    def __init__(self, n_estimators=200, mode='regression', \n",
    "                  learning_rate=0.1, max_depth=3, min_samples_split=5 ):\n",
    "        \n",
    "        #Parameters\n",
    "        self.n_estimators = n_estimators              # <-- No. of predictors\n",
    "        self.mode=mode                                # <-- 'regression'|'classification'\n",
    "        self.learning_rate=learning_rate              # <-- uniform value of alpha defined for each predictor\n",
    "        self.max_depth=max_depth                      # <-- max_depth of decision trees used as predictors\n",
    "        self.min_samples_split=min_samples_split      # <-- min. samples required to split the node of a decision tree\n",
    "        \n",
    "        self.trained_models=[]                        # <-- List of predictors to be used\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        models=[]\n",
    "        tree_parameters = {'max_depth': self.max_depth, 'min_samples_split': self.min_samples_split}\n",
    "        \n",
    "        for i in range(0, self.n_estimators-1):    # <-- After including h0, we need (n_estimators-1) Decision Trees\n",
    "            models.append(DecisionTreeRegressor(**tree_parameters)) # h1(x) and onwards -> DecisionTreeRegressors\n",
    "        \n",
    "        #print(\"y :\", y)\n",
    "        \n",
    "        if self.mode=='regression':\n",
    "            #Defining first model h0 as dummy regressor\n",
    "            #h1(x) and onwards are DecisionTreeRegressors\n",
    "            h0 = DummyRegressor()  #DummyRegressor will just predict the mean of training data\n",
    "        elif self.mode=='classification':\n",
    "            #Defining first model h0 as dummy classifier\n",
    "            #h1(x) and onwards are DecisionTreeRegressors\n",
    "            h0 = DummyClassifier()  #DummyClassifier will just predict the prior probabilities of training data\n",
    "            \n",
    "        h0.fit(X, y) # fit the dummy model\n",
    "        self.trained_models.append(h0)  # First trained model h0 appended\n",
    "        \n",
    "        # For every model in list, first train using all previously trained models \n",
    "        # then appending to the trained model list\n",
    "        for model in models:       \n",
    "            \n",
    "            y_pred = self.predict(X, final=False)   #First loop will just have h0 model, second will have h0&h1, \n",
    "                                                    #third will have h0,h1,h2 for prediction and so on\n",
    "            \n",
    "            #print(\"y_pred: \", y_pred[:3])\n",
    "            \n",
    "            #residual will be the total errors made by trained_models\n",
    "            #First loop will have y-h0, second will have y-(h0+a1*h1), third will have y-(h0+a1*h1+a2*h2) and so on.\n",
    "            #As models are added to the trained_models list after fitting on the residuals, we will see decreasing\n",
    "            #value of residuals in the next iterations.\n",
    "            residual = self.grad(y, y_pred)        #returns y - y_pred -> error OR -ive of Gradient\n",
    "            \n",
    "            model.fit(X, residual)              #Using fit method of DecisionTreeRegressor class to fit the model to\n",
    "                                                #predict the residual value. This way we can predict the residual and \n",
    "                                                #add it to our y_pred value to reduce error. Each subsequent model is \n",
    "                                                #trained to predict a residual value which is decreasing with each \n",
    "                                                #addition of a new predictor. This way the residual --> error is \n",
    "                                                #reduced to a very small value.\n",
    "                                \n",
    "                                                #Hyperparameters for the above operation may be alpha=learning_rate,\n",
    "                                                #n_estimators = no. of predictors, and max_depth of the Decision Trees.\n",
    "            \n",
    "            self.trained_models.append(model)      #Add to trained models list to be used in next iteration --> to\n",
    "                                                   #train the next model\n",
    "    def predict(self, X, final=True):\n",
    "        \n",
    "        #We first get a value using our base predictor which is a DummyRegressor \n",
    "        #because it is not multiplied by alpha=learning_rate\n",
    "        base_predictor = self.trained_models[0].predict(X)\n",
    "        \n",
    "        #Initialize the sum of residuals to be zero\n",
    "        boost_predictors = 0\n",
    "        \n",
    "        #Predict residual error using DecisionTreeRegressors\n",
    "        for model in self.trained_models[1:]:\n",
    "            boost_predictors += self.learning_rate * model.predict(X)\n",
    "        if self.mode == 'regression':\n",
    "            #Return y_predicted = h0 + residuals\n",
    "            return base_predictor + boost_predictors\n",
    "        elif self.mode == 'classification' and final==False:\n",
    "            #Return y_predicted = g( h0 + residuals ) where g(z) is softmax()\n",
    "            return self.softmax(base_predictor + boost_predictors)\n",
    "        elif self.mode == 'classification' and final==True:\n",
    "            #Return y_predicted = argmax( g( h0 + residuals ) )  where g(z) is softmax()\n",
    "            return self.softmax(base_predictor + boost_predictors).argmax(axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Please enter valid mode: 'regression' or 'classification'\")\n",
    "        \n",
    "    def grad(self, y, y_pred):\n",
    "        return y - y_pred     # -ive gradient w.r.t h(x) --> also referred to as residual (+ive gradient is y_pred-y)\n",
    "                              # We need gradient w.r.t h(x) as we are updating the h(x) function\n",
    "                              # Previously we used gradient=X.T@(y_pred-y) which was w.r.t theta since we updated\n",
    "                              # the value of theta using gradient\n",
    "    \n",
    "    def softmax(self, z):  # <--- Softmax function works for both binary and multinomial classification\n",
    "        return np.exp(z)/( np.sum( np.exp(z), axis=1, keepdims=True ) )   # <---- Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data ----> Regression\n",
    "X, y = load_boston(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da69a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  6.722540835508793\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Regression\n",
    "parameters = {'n_estimators': 200, 'mode': 'regression', \n",
    "              'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5} #Good results achieved for max_depth=3\n",
    "model = gradientBoosting(**parameters)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE: \", np.sum(((y_test-yhat)**2)/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ede6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data ----> Binary Classification\n",
    "X, y = load_breast_cancer(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cef70477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_y(y):\n",
    "    n_class = len(np.unique(y))\n",
    "    y_encoded = np.zeros((y.shape[0], n_class))\n",
    "    for _class in range(0, n_class):\n",
    "        y_encoded[np.where(y==_class),_class] = 1 #for indexes where y=class --> y_encoded[selected_idx, class_idx] = 1\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c4b00dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_encoded : [[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Accuracy :  0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Binary Classification\n",
    "parameters = {'n_estimators': 200, 'mode': 'classification', \n",
    "              'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5}\n",
    "y_train=encode_y(y_train)\n",
    "print(\"y_encoded :\", y_train[:5])\n",
    "model = gradientBoosting(**parameters)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "#print(yhat)\n",
    "print(\"Accuracy : \", np.sum(yhat==y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e888bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data ----> Multinomial Classification\n",
    "X, y = load_digits( return_X_y=True )\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad4b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_encoded : [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "Accuracy :  0.9314814814814815\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Multinomial Classification\n",
    "parameters = {'n_estimators': 200, 'mode': 'classification', \n",
    "              'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5}\n",
    "y_train=encode_y(y_train)\n",
    "print(\"y_encoded :\", y_train[:5])\n",
    "model = gradientBoosting(**parameters)\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "#print(yhat)\n",
    "print(\"Accuracy : \", np.sum(yhat==y_test)/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22ba138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_depth: 1, alpha: 0.1, Accuracy :  0.7833333333333333\n",
      "For max_depth: 1, alpha: 0.2, Accuracy :  0.8629629629629629\n",
      "For max_depth: 1, alpha: 0.3, Accuracy :  0.8777777777777778\n",
      "For max_depth: 1, alpha: 0.4, Accuracy :  0.8944444444444445\n",
      "For max_depth: 1, alpha: 0.5, Accuracy :  0.9111111111111111\n",
      "For max_depth: 1, alpha: 0.6, Accuracy :  0.9148148148148149\n",
      "For max_depth: 1, alpha: 0.7, Accuracy :  0.912962962962963\n",
      "For max_depth: 1, alpha: 0.8, Accuracy :  0.9148148148148149\n",
      "For max_depth: 1, alpha: 0.9, Accuracy :  0.9166666666666666\n",
      "For max_depth: 3, alpha: 0.1, Accuracy :  0.9296296296296296\n",
      "For max_depth: 3, alpha: 0.2, Accuracy :  0.9518518518518518\n",
      "For max_depth: 3, alpha: 0.3, Accuracy :  0.9555555555555556\n",
      "For max_depth: 3, alpha: 0.4, Accuracy :  0.9555555555555556\n",
      "For max_depth: 3, alpha: 0.5, Accuracy :  0.9592592592592593\n",
      "For max_depth: 3, alpha: 0.6, Accuracy :  0.9592592592592593\n",
      "For max_depth: 3, alpha: 0.7, Accuracy :  0.9537037037037037\n",
      "For max_depth: 3, alpha: 0.8, Accuracy :  0.9592592592592593\n",
      "For max_depth: 3, alpha: 0.9, Accuracy :  0.9629629629629629\n",
      "For max_depth: 5, alpha: 0.1, Accuracy :  0.9518518518518518\n",
      "For max_depth: 5, alpha: 0.2, Accuracy :  0.9592592592592593\n",
      "For max_depth: 5, alpha: 0.3, Accuracy :  0.9629629629629629\n",
      "For max_depth: 5, alpha: 0.4, Accuracy :  0.9648148148148148\n",
      "For max_depth: 5, alpha: 0.5, Accuracy :  0.9666666666666667\n",
      "For max_depth: 5, alpha: 0.6, Accuracy :  0.9629629629629629\n",
      "For max_depth: 5, alpha: 0.7, Accuracy :  0.9666666666666667\n",
      "For max_depth: 5, alpha: 0.8, Accuracy :  0.9648148148148148\n",
      "For max_depth: 5, alpha: 0.9, Accuracy :  0.9648148148148148\n",
      "For max_depth: 7, alpha: 0.1, Accuracy :  0.9462962962962963\n",
      "For max_depth: 7, alpha: 0.2, Accuracy :  0.9481481481481482\n",
      "For max_depth: 7, alpha: 0.3, Accuracy :  0.9481481481481482\n",
      "For max_depth: 7, alpha: 0.4, Accuracy :  0.95\n",
      "For max_depth: 7, alpha: 0.5, Accuracy :  0.9555555555555556\n",
      "For max_depth: 7, alpha: 0.6, Accuracy :  0.9518518518518518\n",
      "For max_depth: 7, alpha: 0.7, Accuracy :  0.9462962962962963\n",
      "For max_depth: 7, alpha: 0.8, Accuracy :  0.9481481481481482\n",
      "For max_depth: 7, alpha: 0.9, Accuracy :  0.95\n",
      "For max_depth: 9, alpha: 0.1, Accuracy :  0.9166666666666666\n",
      "For max_depth: 9, alpha: 0.2, Accuracy :  0.9185185185185185\n",
      "For max_depth: 9, alpha: 0.3, Accuracy :  0.9111111111111111\n",
      "For max_depth: 9, alpha: 0.4, Accuracy :  0.8962962962962963\n",
      "For max_depth: 9, alpha: 0.5, Accuracy :  0.9037037037037037\n",
      "For max_depth: 9, alpha: 0.6, Accuracy :  0.9074074074074074\n",
      "For max_depth: 9, alpha: 0.7, Accuracy :  0.9074074074074074\n",
      "For max_depth: 9, alpha: 0.8, Accuracy :  0.9185185185185185\n",
      "For max_depth: 9, alpha: 0.9, Accuracy :  0.9074074074074074\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [1, 3, 5, 7, 9]:\n",
    "    for alpha in [i/10 for i in range(1,10)]:\n",
    "        parameters = {'n_estimators': 200, 'mode': 'classification', \n",
    "              'learning_rate': alpha, 'max_depth': max_depth, 'min_samples_split': 5}\n",
    "        model = gradientBoosting(**parameters)\n",
    "        model.fit(X_train, y_train)\n",
    "        yhat = model.predict(X_test)\n",
    "        #print(yhat)\n",
    "        print(f\"For max_depth: {max_depth}, alpha: {alpha}, Accuracy : \", np.sum(yhat==y_test)/y_test.shape[0])\n",
    "        \n",
    "    #BEST RESULTS for max_depth = 5 and learning_rate = 0.5  ---> Accuracy :  0.9666666666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
